# -*- coding: utf-8 -*-
"""hw4-a3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wbvno5kqwLUNqWrbM5XnCSSJbPjiIp0F
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
import numpy as np

from typing import Tuple, Union, List, Callable
from torch.optim import SGD
import torchvision
from torch.utils.data import DataLoader, TensorDataset, random_split
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import random

# %matplotlib inline

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

train_dataset = torchvision.datasets.CIFAR10("./data", train=True, download=True, transform=torchvision.transforms.ToTensor())
test_dataset = torchvision.datasets.CIFAR10("./data", train=False, download=True, transform=torchvision.transforms.ToTensor())

SAMPLE_DATA = False # set this to True if you want to speed up training when searching for hyperparameters!

batch_size = 128

if SAMPLE_DATA:
  train_dataset, _ = random_split(train_dataset, [int(0.1 * len(train_dataset)), int(0.9 * len(train_dataset))]) # get 10% of train dataset and "throw away" the other 90%"""

train_dataset, val_dataset = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])

# Create separate dataloaders for the train, test, and validation set
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=True
)

def fully_connected_model(M: int) -> nn.Module:
  model = nn.Sequential(
      nn.Flatten(),
      nn.Linear(3072, M),
      nn.ReLU(),
      nn.Linear(M, 10),
  )
  return model.to(DEVICE)

def conv_model(M: int=100, k: int=5, N: int=14) -> nn.Module:
  model = nn.Sequential(
      nn.Conv2d(in_channels=3, out_channels=M, kernel_size=k),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=N, stride=N),
      nn.Flatten(),
      nn.Linear(M * ((33-k) // N) ** 2, 10)
  )
  return model.to(DEVICE)

def train(
    model: nn.Module, optimizer: SGD,
    train_loader: DataLoader, val_loader: DataLoader,
    epochs: int
    )-> Tuple[List[float], List[float], List[float], List[float]]:
    """
    Trains a model for the specified number of epochs using the loaders.

    Returns:
    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.
    """

    loss_fn = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for e in tqdm(range(epochs)):
        model.train()
        train_loss = 0.0
        train_acc = 0.0

        # Main training loop; iterate over train_loader. The loop
        # terminates when the train loader finishes iterating, which is one epoch.
        for (x_batch, labels) in train_loader:
            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            labels_pred = model(x_batch)
            batch_loss = loss_fn(labels_pred, labels)
            train_loss = train_loss + batch_loss.item()

            labels_pred_max = torch.argmax(labels_pred, 1)
            batch_acc = torch.sum(labels_pred_max == labels)
            train_acc = train_acc + batch_acc.item()

            batch_loss.backward()
            optimizer.step()

        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc / len(train_loader.dataset))

        # Validation loop; use .no_grad() context manager to save memory.
        model.eval()
        val_loss = 0.0
        val_acc = 0.0

        with torch.no_grad():
            for (v_batch, labels) in val_loader:
                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)
                labels_pred = model(v_batch)
                v_batch_loss = loss_fn(labels_pred, labels)
                val_loss = val_loss + v_batch_loss.item()

                v_pred_max = torch.argmax(labels_pred, 1)
                batch_acc = torch.sum(v_pred_max == labels)
                val_acc = val_acc + batch_acc.item()

        val_losses.append(val_loss / len(val_loader))
        val_accuracies.append(val_acc / len(val_loader.dataset))

        print(f"Epoch {e+1}/{epochs}:")
        print(f"  Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.4f}")

    return train_losses, train_accuracies, val_losses, val_accuracies

def parameter_search_fullyconnected(train_loader: DataLoader,
                     val_loader: DataLoader,
                     model_fn:Callable[[], nn.Module],
                     threshold: float,
                     epochs: int,
                     momentum: float = 0.9) -> Tuple[float, int, dict]:
    """
    Parameter search for our linear model using SGD.

    Args:
    train_loader: the train dataloader.
    val_loader: the validation dataloader.
    model_fn: a function that, when called, returns a torch.nn.Module.

    Returns:
    The learning rate with the least validation loss.
    NOTE: you may need to modify this function to search over and return
     other parameters beyond learning rate.
    """

    best_val_acc = 0.0
    best_lr = 0.0
    best_M = 0
    best_model = None
    lrs = torch.logspace(-6, -1, 10).tolist()
    hidden = [400, 500, 700, 800, 900, 1000]
    results = {}

    for lr in lrs:
        for M in hidden:
          print(f"trying learning rate {lr} with hidden layer size {M}")
          model = model_fn(M)
          optim = SGD(model.parameters(), lr, momentum)
          train_loss, train_acc, val_loss, val_acc = train(model, optim, train_loader, val_loader, epochs)

          results[(M, lr)] = (train_acc, val_acc)

          max_val_acc = max(val_acc)
          if max_val_acc > best_val_acc:
              best_val_acc = max_val_acc
              best_lr = lr
              best_M = M
              best_model = model

          if best_val_acc >= threshold:
            print("passed the threshold!")
            break

    return best_lr, best_M, results, best_model

def parameter_search_convolutional(train_loader: DataLoader,
                     val_loader: DataLoader,
                     model_fn:Callable[[], nn.Module],
                     threshold: float,
                     epochs: int,
                     momentum: float = 0.9) -> Tuple[float, int, dict]:
    """
    Parameter search for our linear model using SGD.

    Args:
    train_loader: the train dataloader.
    val_loader: the validation dataloader.
    model_fn: a function that, when called, returns a torch.nn.Module.

    Returns:
    The learning rate with the least validation loss.
    NOTE: you may need to modify this function to search over and return
     other parameters beyond learning rate.
    """

    best_val_acc = 0.0
    best_lr = 0.0
    best_M = 0
    best_k = 0
    best_N = 0
    best_model = None
    lrs = torch.logspace(-6, -1, 10).tolist()
    M_set = [100, 200, 1000]
    k_set = [5]
    N_set = [10, 12, 14]
    results = {}

    for lr in lrs:
        for M in M_set:
            for k in k_set:
              for N in N_set:
                print(f"Trying lr={lr}, M={M}, k={k}, N={N}")
                model = model_fn(M, k, N)
                optim = SGD(model.parameters(), lr, momentum)
                train_loss, train_acc, val_loss, val_acc = train(model, optim, train_loader, val_loader, epochs)

                results[(M, k, N, lr)] = (train_acc, val_acc)

                max_val_acc = max(val_acc)
                if max_val_acc > best_val_acc:
                    best_val_acc = max_val_acc
                    best_lr = lr
                    best_M = M
                    best_k = k
                    best_N = N
                    best_model = model

                if best_val_acc >= threshold:
                  print("passed the threshold!")
                  break

    return best_lr, best_M, best_k, best_N, results, best_model

def plot_fullyconnected(results: dict, threshold: float):
  top_results = sorted(results.items(), key=lambda x: max(x[1][1]), reverse=True)[:3]

  plt.figure(figsize=(12, 6))
  for key, (train_acc, val_acc) in top_results:
      M, lr = key
      model = fully_connected_model(M)
      optimizer = SGD(model.parameters(), lr)
      train_loss, train_accuracy, val_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, 60)
      epochs = range(1, len(train_acc) + 1)
      plt.plot(epochs, [acc.cpu().numpy() if torch.is_tensor(acc) else acc for acc in train_acc], label=f'Train Hidden Size={M}, Learning Rate={lr}', linestyle='-')
      plt.plot(epochs, [acc.cpu().numpy() if torch.is_tensor(acc) else acc for acc in val_acc], label=f'Val Hidden Size={M}, Learning Rate={lr}', linestyle='--')
  plt.axhline(y=threshold, color='r', linestyle='-', label=f'Threshold: {threshold*100}%')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.title('Training and Validation Classification Accuracy for Top Hyperparameter Configurations')
  plt.show()

def plot_convolutional(results: dict, threshold: float):
  top_results = sorted(results.items(), key=lambda x: max(x[1][1]), reverse=True)[:3]

  plt.figure(figsize=(12, 6))
  for key, (train_acc, val_acc) in top_results:
      M, k, N, lr = key
      model = conv_model(M, k, N)
      optimizer = SGD(model.parameters(), lr)
      train_loss, train_accuracy, val_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, 60)
      epochs = range(1, len(train_acc) + 1)
      plt.plot(epochs, [acc.cpu().numpy() if torch.is_tensor(acc) else acc for acc in train_acc], label=f'Train Hidden Size={M}, Learning Rate={lr}', linestyle='-')
      plt.plot(epochs, [acc.cpu().numpy() if torch.is_tensor(acc) else acc for acc in val_acc], label=f'Val Hidden Size={M}, Learning Rate={lr}', linestyle='--')
  plt.axhline(y=threshold, color='r', linestyle='-', label=f'Threshold: {threshold*100}%')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.title('Training and Validation Classification Accuracy for Top Hyperparameter Configurations')
  plt.show()

def evaluate(
    model: nn.Module, loader: DataLoader
) -> Tuple[float, float]:
    """Computes test loss and accuracy of model on loader."""
    loss = nn.CrossEntropyLoss()
    model.eval()
    test_loss = 0.0
    test_acc = 0.0
    with torch.no_grad():
        for (batch, labels) in loader:
            batch, labels = batch.to(DEVICE), labels.to(DEVICE)
            y_batch_pred = model(batch)
            batch_loss = loss(y_batch_pred, labels)
            test_loss = test_loss + batch_loss.item()

            pred_max = torch.argmax(y_batch_pred, 1)
            batch_acc = torch.sum(pred_max == labels)
            test_acc = test_acc + batch_acc.item()
        test_loss = test_loss / len(loader)
        test_acc = test_acc / (batch_size * len(loader))
        return test_loss, test_acc

best_lr, best_M, results, best_model = parameter_search_fullyconnected(train_loader, val_loader, fully_connected_model, threshold=0.5, epochs=20, momentum=0.9)
print("Fully-connected output, 1 fully-connected hidden layer.")
print(f"Best hidden size: {best_M}, Best learning rate: {best_lr}")

plot_fullyconnected(results, threshold=0.5)

_, test_acc = evaluate(best_model, test_loader)
print(f"Test Accuracy: {test_acc:.4f}")

best_lr, best_M, best_k, best_N, results, best_model = parameter_search_convolutional(train_loader, val_loader, conv_model, threshold=0.65, epochs=20, momentum=0.9)
print("Convolutional layer with max-pool and fully-connected output.")
print(f"Best number of filters: {best_M}, Best filter size: {best_k}, Best pooling size: {best_N}, Best learning rate: {best_lr}")

plot_convolutional(results, threshold=0.65)

_, test_acc = evaluate(best_model, test_loader)
print(f"Test Accuracy: {test_acc:.4f}")